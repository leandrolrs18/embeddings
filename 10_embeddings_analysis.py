# -*- coding: utf-8 -*-
"""10 - Embeddings Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l6UfBDpA2-xZehDj6ljO4aYlQOZssjTl

# Análise de Embeddings e Redução da Dimensionalidade

**Objetivo.** Dado um conjunto de textos, gerar embeddings com BERT e investigar a estrutura dos dados via PCA, t-SNE e UMAP. Em seguida, identificar clusters e relacioná-los a categorias semânticas.
"""

sentences = [
    'I swap butter for olive oil in many recipes.',
    'Canberra is the capital of Australia.',
    'Ottawa is the capital city of Canada.',
    'Paris is the most populated city in France.',
    'Tokyo is among the most populous metropolitan areas worldwide.',
    'I prefer my coffee with no sugar and a splash of milk.',
    'The recipe for pasta carbonara is simple.',
    'A pinch of salt enhances sweetness in desserts.',
    'Alignment techniques reduce harmful outputs.',
    'Explainable AI highlights salient features for decisions.',
    'Transformer models enable long-range language dependencies.',
    'Black swan events stress-test portfolio resilience.',
    'The Sahara Desert spans much of North Africa.',
    'Inflation erodes real purchasing power of cash.',
    'Aromatics like garlic and onion build flavor early.',
    'Value stocks trade at lower multiples relative to fundamentals.',
    'Quantization reduces memory with minimal accuracy loss.',
    'Tax-loss harvesting offsets capital gains.',
    'Investing in technology can be risky.',
    'Fermented foods add acidity and complexity.',
    'Marinating tofu improves texture and taste.',
    'Vector databases power semantic search at scale.',
    'Distillation transfers knowledge from large to small models.',
    'The Great Barrier Reef lies off Australia’s northeast coast.',
    'Retrieval-augmented generation grounds answers in sources.',
    'Iceland lies on the Mid-Atlantic Ridge.',
    'The Baltic states border the eastern Baltic Sea.',
    'Multimodal learning aligns text with images and audio.',
    'Risk tolerance should guide position sizing.',
    'Time in the market beats timing the market.',
    'Behavioral biases can derail investment plans.',
    'Reinforcement learning fine-tunes policies from human feedback.',
    'Edge AI runs models under strict latency constraints.',
    'Deglazing lifts browned bits to make pan sauces.',
    'Tempering chocolate stabilizes cocoa butter crystals.',
    'What is the capital of France?',
    'Johannesburg is a major city but not South Africa’s capital.',
    'The Danube passes through multiple European capitals.',
    'The Amazon River carries one of the largest water volumes on Earth.',
    'A healthy emergency fund reduces forced selling.',
    'I batch-cook grains for quick lunches.',
    'Resting steak helps redistribute the juices.',
    'The Atacama is one of the driest deserts on the planet.',
    'Liquidity risk rises when trading volumes are thin.',
    'Mount Everest is the highest peak above sea level.',
    'Graph neural networks capture relational structure.',
    'Sourdough starter needs regular feedings to stay active.',
    'The stock market experienced a drop today.',
    'Umami-rich ingredients deepen savory dishes.',
    'Al dente pasta retains a slight bite after cooking.',
    'Rebalancing restores target asset allocation.',
    'Continual learning mitigates catastrophic forgetting.',
    'Bond duration measures sensitivity to interest-rate changes.',
    'Diffusion models synthesize high-fidelity images.',
    'Expense ratios compound against long-term returns.',
    'Self-supervised pretraining reduces labeled data needs.',
    'What country contains the city of Kyoto?',
    'Stir-frying requires high heat and constant movement.',
    'Covered calls generate income with capped upside.',
    'The Nile flows northward into the Mediterranean Sea.',
    'Causal inference distinguishes correlation from effect.',
    'Prompt engineering steers generative behavior reliably.',
    'Few-shot prompting improves generalization on new tasks.',
    'Growth investing prioritizes earnings expansion.',
    'The Alps stretch across several central European countries.',
    'The Andes form a continuous mountain range along South America.',
    'I cook vegetarian meals on weekdays to simplify planning.',
    'Natural language processing has advanced greatly.',
    'Sous-vide delivers precise temperature control.',
    'Diversification reduces idiosyncratic risk across holdings.',
    'Sharpe ratio evaluates risk-adjusted performance.',
    'Artificial intelligence is transforming the world.',
    'Credit spreads widen during economic uncertainty.',
    'Emerging markets add diversification but higher volatility.',
    'Mise en place speeds up weeknight cooking.',
    'The Caspian Sea is a landlocked body of water.',
    'Evaluation with benchmarks must avoid data leakage.',
    'Cairo sits along the Nile River delta.',
    'Federated learning trains models without centralizing data.',
    'Lagos is Nigeria’s largest city by population.',
    'Dollar-cost averaging smooths entry price over time.',
    'LoRA adapters enable efficient fine-tuning.',
    'I keep a jar of homemade pesto for pasta.',
    'New Delhi serves as the seat of India’s government.',
    'I like to cook Italian dishes on Sundays.',
    'Roasting vegetables caramelizes natural sugars.',
    'ETFs provide broad market exposure with intraday liquidity.',
    'Proofing time affects a bread’s crumb structure.'
]

"""## Predição dos Embeddings

Utilize o modelo BERT pré-treinado para gerar embeddings de todos os textos fornecidos.  
O objetivo é obter uma matriz `X` com formato **(N, dim)**, onde **N** é o número de textos e **dim** é a dimensionalidade dos vetores de embedding.
"""

# !pip install sentence-transformers torch scikit-learn matplotlib seaborn pandas umap-learn -q

import torch
import numpy as np
from sentence_transformers import SentenceTransformer #classe principal para carregar modelos de embeddings

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = 'sentence-transformers/all-MiniLM-L6-v2'
model = SentenceTransformer(model_name, device=device)
sentence_embeddings = model.encode(sentences) # Gera os embeddings (vetores numéricos) para todas as frases da lista sentences.
#prepara o tokenizer
#configura CPU/GPU e batch size

print(f"Shape da matriz de embeddings: {sentence_embeddings.shape}")

from sklearn.metrics.pairwise import cosine_similarity

query = "Planets similar to Earth"
query_embedding = model.encode([query])
print(f"Shape da matriz de embeddings: {query_embedding.shape}")
similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]
k = 2
top_k_indices = np.argsort(similarities)[::-1][:k]

print(f"Consulta: '{query}'\n")
print(f"Top {k} sentenças mais similares no corpus:")
for index in top_k_indices:
    print(f"  - {sentences[index]} (Score: {similarities[index]:.4f})")

"""## PCA

Aplique **PCA (Principal Component Analysis)** para projetar os embeddings em duas dimensões e visualizar a estrutura global dos dados.  
O PCA ajuda a capturar as direções de maior variância e pode indicar agrupamentos lineares.

**Tarefas:**
- Reduza a dimensionalidade dos embeddings para 2 componentes principais.  
- Plote os pontos resultantes com `matplotlib`, identificando possíveis agrupamentos.  
- Analise qualitativamente se há separação entre textos de temas distintos.
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

pca = PCA(n_components=2)

embeddings_2d_pca = pca.fit_transform(sentence_embeddings)

print(embeddings_2d_pca.shape)

# Cria um DataFrame para a plotagem
df_pca = pd.DataFrame({
    'x': embeddings_2d_pca[:, 0],
    'y': embeddings_2d_pca[:, 1],
    'sentence': sentences
})

# Plota os resultados
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df_pca, x='x', y='y', s=150)

# Adiciona anotações aos pontos
for i in range(df_pca.shape[0]):
    plt.text(
        df_pca.x[i],
        df_pca.y[i],
        df_pca.sentence[i],
        ha='left',
        size=10,
        color='black',
        alpha=1.0,
        weight='normal'
    )

plt.title('Visualização de Embeddings com t-SNE')
plt.xlabel('Componente t-SNE 1')
plt.ylabel('Componente t-SNE 2')
plt.grid(True)
plt.show()

"""## t-SNE

Use **t-SNE (t-distributed Stochastic Neighbor Embedding)** para investigar a estrutura local dos dados.  
Diferente do PCA, o t-SNE tenta preservar vizinhanças locais e pode revelar grupos mais sutis.

**Tarefas:**
- Reduza os embeddings para 2D usando `TSNE` do `scikit-learn`.  
- Ajuste parâmetros como `perplexity` e `learning_rate` para comparar resultados.  
- Visualize o mapa e observe se os textos semelhantes ficam próximos.
"""

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


tsne = TSNE(n_components=2, perplexity=50, random_state=42, init='random', learning_rate=200)
embeddings_2d_tsne = tsne.fit_transform(sentence_embeddings)

print(f"Shape da matriz de embeddings_2d_tsne: {embeddings_2d_tsne.shape}")

# Cria um DataFrame para a plotagem
df_tsne = pd.DataFrame({
    'x': embeddings_2d_tsne[:, 0],
    'y': embeddings_2d_tsne[:, 1],
    'sentence': sentences
})

# Plota os resultados
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df_tsne, x='x', y='y', s=150)

# Adiciona anotações aos pontos
for i in range(df_tsne.shape[0]):
    plt.text(
        df_tsne.x[i],
        df_tsne.y[i],
        df_tsne.sentence[i],
        ha='left',
        size=10,
        color='black',
        alpha=1.0,
        weight='normal'
    )

plt.title('Visualização de Embeddings com t-SNE')
plt.xlabel('Componente t-SNE 1')
plt.ylabel('Componente t-SNE 2')
plt.grid(True)
plt.show()

"""## UMAP

Aplique **UMAP (Uniform Manifold Approximation and Projection)** como alternativa ao t-SNE.  
O UMAP é mais eficiente, preserva parte da estrutura global e é útil para visualização e pré-processamento.

**Tarefas:**
- Gere uma projeção 2D dos embeddings com `umap.UMAP`.  
- Experimente variar `n_neighbors` e `min_dist` para observar mudanças na distribuição dos clusters.  
- Compare visualmente com os resultados do PCA e t-SNE.
"""

from umap import UMAP
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

umap = UMAP(
    n_components=2, #nuvem de pontos ficará em 2 dimensões
    n_neighbors=20, #quanto da estrutura global/local o UMAP tenta preservar
    min_dist=0.009, #o quão apertados os clusters ficam no gráfico
    random_state=42 #número qualquer
)

embeddings_2d_umap = umap.fit_transform(sentence_embeddings)

print(f"Shape da matriz embeddings_2d_umap: {embeddings_2d_umap.shape}")

df_umap = pd.DataFrame({
    'x': embeddings_2d_umap[:, 0],
    'y': embeddings_2d_umap[:, 1],
    'sentence': sentences
})

# Plota os resultados
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df_umap, x='x', y='y', s=150)

# Adiciona anotações aos pontos
for i in range(df_umap.shape[0]):
    plt.text(
        df_umap.x[i],
        df_umap.y[i],
        df_umap.sentence[i],
        ha='left',
        size=10,
        color='black',
        alpha=1.0,
        weight='normal'
    )

plt.title('Visualização de Embeddings com t-SNE')
plt.xlabel('Componente t-SNE 1')
plt.ylabel('Componente t-SNE 2')
plt.grid(True)
plt.show()

"""## Classificação

Com base nas categorias observadas nos gráficos anteriores, crie uma função simples que receba um texto e classifique-o na categoria mais provável.

**Tarefas:**
- Use os embeddings existentes e os clusters identificados para rotular automaticamente cada texto.  
- Crie uma função `classificar_texto(texto: str)` que:
  1. Gere o embedding do texto.
  2. Calcule a distância para os clusters identificados.
  3. Retorne o nome do cluster mais próximo.
"""

from sklearn.cluster import KMeans
import numpy as np


kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(sentence_embeddings)

centroids = kmeans.cluster_centers_

CLUSTER_NAMES = {
    0: "Cooking",
    1: "Finance",
    2: "Geography",
    3: "Technology"
}


def classify_text(text: str) -> str:
    emb = model.encode([text])[0]

    sims = cosine_similarity([emb], centroids)[0]
    cluster_id = int(np.argmax(sims))

    return CLUSTER_NAMES.get(cluster_id, f"Cluster {cluster_id}")


COOKING_TEST = [
    "How do I properly boil pasta?",
    "What spices should I use for curry?",
    "How do I caramelize onions?",
    "What temperature should I bake chicken at?",
]

FINANCE_TEST = [
    "How does compound interest work?",
    "What is a diversified investment portfolio?",
    "Why do interest rates affect mortgages?",
    "What causes stock market volatility?",
]

GEOGRAPHY_TEST = [
    "What is the capital of Norway?",
    "Where is the Sahara Desert located?",
    "Which river flows through Paris?",
    "Where is the Great Barrier Reef?",
]

TECHNOLOGY_TEST = [
    "What is a neural network?",
    "How does Wi-Fi communication work?",
    "What does a CPU do?",
    "What is machine learning?",
]



def run_tests():
    for t in COOKING_TEST:
        print(t, "→", classify_text(t))

    for t in FINANCE_TEST:
        print(t, "→", classify_text(t))

    for t in GEOGRAPHY_TEST:
        print(t, "→", classify_text(t))

    for t in TECHNOLOGY_TEST:
        print(t, "→", classify_text(t))

run_tests()